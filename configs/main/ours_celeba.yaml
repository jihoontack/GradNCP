{
  # meta-learning configs
  "mode": "maml_bootstrap_param",
  "lam": 100.0,
  "inner_lr_boot": 5.e-2,

  "inner_steps": 16,
  "inner_steps_test": 16,
  "lr": 1.e-5,
  "inner_lr": 1.e-2,
  "batch_size": 8,
  "test_batch_size": 2,
  "outer_steps": 150000,
  "dataset": 'celeba',
  "data_ratio": 0.25,
  "sample_type": 'gradncp',
  "no_date": True,

  # model configs
  "decoder": 'siren',
  "num_layers": 5,
  "dim_in": 2,
  "dim_out": 3,
  "dim_hidden": 256,

}